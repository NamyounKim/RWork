# Network edge size 값 설정하기 (단어간 상관계수 값 * 2)
set.edge.value(net, "edgeSize", cor_termW * 2)
# Network Map 화면에 그리기
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic")
#TF-IDF 값으로 연관 키워드 추출하기
dtmW = DocumentTermMatrix(corp, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE))) #Tf-Idf 가중치 주기
## 단어 양옆 스페이스 제거 및 한글자 단어 제외하기
colnames(dtmW) = trimws(colnames(dtmW))
dtmW = dtmW[,nchar(colnames(dtmW)) > 1]
dtmW = removeSparseTerms(dtmW, as.numeric(0.97))
dtmW
dtmW
dtmW_m = as.matrix(dtmW)
cor_termW = cor(dtmW_m)
dtmW_m = as.matrix(dtmW)
cor_termW = cor(dtmW_m)
cor_termW[cor_termW < 0.25] = 0
net = network(cor_termW, directed = FALSE)
net %v% "mode" = ifelse(betweenness(net) > quantile(betweenness(net), 0.9), "big", "small")
node_color = c("small" = "grey", "big" = "gold")
set.edge.value(net, "edgeSize", cor_termW * 2)
# Network Map 화면에 그리기
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,layout.par = "circrand")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,layout.par = "circrand")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,layout.par = "geodist")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic")
word_network = data.frame(word = rownames(cor_termW),
centrality = degree(net),
betweenness = betweenness(net),
eigenvector = evcent(net))
word_network
keyword = c("정수기","추천","할인")
sub_cor_term = cor_termW[,keyword]
head(sub_cor_term)
keyword = c("정수기","추천","할인")
sub_cor_term = sub_cor_term[!(rownames(sub_cor_term) %in% keyword),]
head(sub_cor_term)
sub_cor_term = sub_cor_term[rowSums(sub_cor_term)>0,]
head(sub_cor_term)
head(sub_cor_term)
net2 = network(sub_cor_term, directed = FALSE, matrix.type="bipartite")
ggnet2(net2 # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,edge.size = sub_cor_term[sub_cor_term>0] * 2
,size = degree(net2) # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,family="AppleGothic")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,mode = "circle")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,mode = "kamadakawai")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,mode = "fruchtermanreingold")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,mode = "circrand")
install.packages("tsne")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,mode = "circle")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,mode = "kamadakawai")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,mode = "fruchtermanreingold")
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic"
,mode = "circrand")
library(devtools)   #Rtools는 Windows에 깔때 별도로 깔아야 한다.
install_github("bmschmidt/wordVectors")
library(wordVectors)
library(tsne)
library(readr)
parsedData = text_parser(path = "./HomeApplication_cafe.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
parsedData
parsedData[1]
write.table(parsedData, file = "./trainTxt.txt", row.names = FALSE, col.names = FALSE, quote = F)
write.table(parsedData, file = "./trainTxt.txt", row.names = FALSE, col.names = FALSE, quote = F)
model = train_word2vec("./trainTxt.txt", output_file = "w2vModel.bin",
threads=3, vectors=100, force = T)
model = train_word2vec("./trainTxt.txt", output_file = "w2vModel.bin",
threads=3, vectors=100, force = T)
read.vectors("./w2vModel.bin")
read.vectors("./w2vModel.bin")
read.vectors("./w2vModel.bin")
nearest_to(model, model[["냉장고"]], 20)
nearest_to(model, model[["냉장고"]], 20)
nearest_to(model,model[[c("냉장고","양문")]], 20)
subVec = model[rownames(model)=="냉장고",] - model[rownames(model) == "디오스",] + model[rownames(model) == "트롬",]
nearest_to(model, subVec, 20)
plot(model)
plot(model)
cosineSimilarity(model[["냉장고"]], model[["그룹"]])
dist(model[(row.names(model)=="냉장고" | row.names(model)=="그룹"),])
library(tm)
library(slam)
library(dplyr)
library(readr)
library(NLP4kec)
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_131")
library(tm)
library(slam)
library(dplyr)
library(readr)
library(NLP4kec)
library(tm)
library(slam)
library(dplyr)
library(readr)
library(NLP4kec)
parsedData = text_parser(path = "./HomeApplication_cafe.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
parsedData = text_parser_file(path = "./HomeApplication_cafe.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
shiny::runApp('shiny_code')
read.vectors("./w2vModel.bin")
model = train_word2vec("./trainTxt.txt", output_file = "w2vModel.bin",
threads=3, vectors=100, force = T)
library(devtools)   #Rtools는 Windows에 깔때 별도로 깔아야 한다.
library(wordVectors)
library(tsne)
library(readr)
model = train_word2vec("./trainTxt.txt", output_file = "w2vModel.bin",
threads=3, vectors=100, force = T)
read.vectors("./w2vModel.bin")
nearest_to(model,model[["냉장고"]], 20)
temp = cosineDist(model, model)
temp
View(temp)
temp = temp[nchar(rownames(temp)) > 1, nchar(colnames(temp)) > 1]
temp[temp < 1.1] = 0
temp = temp[,colSums(temp)!=0]
temp = temp[rowSums(temp)!=0,]
net = network(temp, directed = FALSE)
net %v% "mode" <- ifelse(betweenness(net) > quantile(betweenness(net), 0.9), "big", "small")
node_color = c("small" = "grey", "big" = "gold")
set.edge.value(net, "edgeSize", temp * 0.8)
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic")
library(topicmodels)
library(topicmodels)
library(topicmodels)
library(LDAvis)
library(servr)
library(readr)
parsedData = text_parser(path = "./GitHub/RWork/FastCampus/HomeApplication_cafe.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
parsedData = text_parser(path = "./HomeApplication_cafe.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
parsedData = gsub(" ","  ",parsedData)
corp=VCorpus(VectorSource(parsedData))
corp = tm_map(corp, removePunctuation)
corp = tm_map(corp, tolower)
corp = tm_map(corp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))
for (j in seq(corp))
{
corp[[j]] = gsub("lg", "엘지", corp[[j]])
corp[[j]] = gsub("samsung", "삼성", corp[[j]])
}
##################################################################
#텍스트문서 형식으로 변환
corp = tm_map(corp, PlainTextDocument)
#Document Term Matrix 생성 (단어 Length는 2로 세팅)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
## 한글자 단어 제외하기 ##
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
#Sparse Terms 삭제
dtm = removeSparseTerms(dtm, as.numeric(0.997))
## LDA 할 때 DTM 크기 조절
#단어별 Tf-Idf 값 구하기
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
#박스그래프로 분포 확인
boxplot(term_tfidf)
# Tf-Idf 값 기준으로 dtm 크기 줄여서 new_dtm 만들기
new_dtm = dtm[,term_tfidf >= 0.1]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]
dtm
dtm$i
row_sums(dtm)
name = "HomeApplication"
SEED = 2017
k = 10 #클러스터 개수 세팅
lda_tm = LDA(new_dtm, control=list(seed=SEED), k)
term_topic = terms(lda_tm, 30)
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
filePathName = paste0("./LDA_output2/",name,"_",k,"_LDA_Result.csv")
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
doc_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
doc_topic_df
head(doc_topic_df)
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)
head(doc_Prob_df)
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
head(doc_Prob_df)
doc_Prob_df$rown = doc_topic_df$rown
head(doc_Prob_df)
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
head(parsedData)
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","doc_topic","maxProb"))
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
write.table(id_topic, filePathName, sep=",", row.names=FALSE)
posterior(lda_tm)$terms
phi = posterior(lda_tm)$terms %>% as.matrix
theta = posterior(lda_tm)$topics %>% as.matrix
vocab = colnames(phi)
doc_length = vector()
doc_topic_df=as.data.frame(doc_topic)
for( i in as.numeric(row.names(doc_topic_df))){
temp = corp[[i]]$content
doc_length = c(doc_length, nchar(temp[1]))
}
# 각 단어별 빈도수를 구합니다.
new_dtm_m = as.matrix(new_dtm)
freq_matrix = data.frame(ST = colnames(new_dtm_m),
Freq = colSums(new_dtm_m))
# 위에서 구한 값들을 파라메터 값으로 넘겨서 시각화를 하기 위한 데이터를 만들어 줍니다.
source("./createNamJson_v2.R")
json_lda = createNamJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA가 작동 안할 때 사용
mds.method = canberraPCA
)
source("./Week_5/createNamJson_v2.R")
json_lda = createNamJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA가 작동 안할 때 사용
mds.method = canberraPCA
)
serVis(json_lda, out.dir = paste("C:/apache-tomcat-8.5.11/webapps/",name,"_",k,sep=""), open.browser = FALSE)
install.packages("topicmodels")
install.packages("LDAvis")
install.packages("servr")
library(topicmodels)
library(LDAvis)
library(servr)
library(readr)
library(tm)
library(slam)
library(dplyr)
library(NLP4kec)
#형태소 분석기 실행하기
parsedData = text_parser(path = "./HomeApplication_cafe.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
## 단어간 스페이스 하나 더 추가하기 ##
parsedData = gsub(" ","  ",parsedData)
##################################################################
#Text Pre-processing
##################################################################
#Corpus 생성
corp=VCorpus(VectorSource(parsedData))
#corp=VCorpus(VectorSource(parsedDataRe$parsedContent))
#특수문자 제거
corp = tm_map(corp, removePunctuation)
#소문자로 변경
corp = tm_map(corp, tolower)
#특정 단어 삭제
corp = tm_map(corp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))
#동의어 처리
for (j in seq(corp))
{
corp[[j]] = gsub("lg", "엘지", corp[[j]])
corp[[j]] = gsub("samsung", "삼성", corp[[j]])
}
##################################################################
#텍스트문서 형식으로 변환
corp = tm_map(corp, PlainTextDocument)
#Document Term Matrix 생성 (단어 Length는 2로 세팅)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
## 한글자 단어 제외하기 ##
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
#Sparse Terms 삭제
dtm = removeSparseTerms(dtm, as.numeric(0.997))
dtm
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
term_tfidf
head(term_tfidf)
boxplot(term_tfidf)
boxplot(term_tfidf)
boxplot(term_tfidf)
new_dtm = dtm[,term_tfidf >= 0.1]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]
new_dtm
name = "HomeApplication"
SEED = 2017
k = 10 #클러스터 개수 세팅
lda_tm = LDA(new_dtm, control=list(seed=SEED), k)
lda_tm
term_topic = terms(lda_tm, 30)
term_topic
head(term_topic)
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
filePathName
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
doc_topic = topics(lda_tm, 1)
head(doc_topic)
doc_topic = topics(lda_tm, 2)
head(doc_topic)
head(doc_topic)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
doc_Prob_df$rown = doc_topic_df$rown
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","doc_topic","maxProb"))
doc_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
doc_Prob_df$rown = doc_topic_df$rown
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","doc_topic","maxProb"))
head(id_topic)
doc_topic
doc_topic_df = as.data.frame(doc_topic)
head(doc_topic_df)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
head(doc_topic_df)
doc_Prob = posterior(lda_tm)$topics
doc_Prob
head(doc_Prob)
doc_Prob_df = as.data.frame(doc_Prob)
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
head(doc_Prob_df)
doc_Prob_df$rown = doc_topic_df$rown
head(doc_Prob_df)
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
head(parsedData)
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
head(id_topic)
id_topic = subset(id_topic,select=c("rown","doc_topic","maxProb"))
head(id_topic)
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
write.table(id_topic, filePathName, sep=",", row.names=FALSE)
posterior(lda_tm)$terms
# phi는 각 단어별 토픽에 포함될 확률값 입니다.
phi = posterior(lda_tm)$terms %>% as.matrix
# theta는 각 문서별 토픽에 포함될 확률값 입니다.
theta = posterior(lda_tm)$topics %>% as.matrix
# vocab는 전체 단어 리스트 입니다.
vocab = colnames(phi)
# 각 문서별 문서 길이를 구합니다.
doc_length = vector()
doc_topic_df=as.data.frame(doc_topic)
for( i in as.numeric(row.names(doc_topic_df))){
temp = corp[[i]]$content
doc_length = c(doc_length, nchar(temp[1]))
}
# 각 단어별 빈도수를 구합니다.
new_dtm_m = as.matrix(new_dtm)
freq_matrix = data.frame(ST = colnames(new_dtm_m),
Freq = colSums(new_dtm_m))
# 위에서 구한 값들을 파라메터 값으로 넘겨서 시각화를 하기 위한 데이터를 만들어 줍니다.
source("./Week_5/createNamJson_v2.R")
json_lda = createNamJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA가 작동 안할 때 사용
mds.method = canberraPCA
)
json_lda
serVis(json_lda, out.dir = paste("C:/apache-tomcat-8.5.11/webapps/",name,"_",k,sep=""), open.browser = FALSE)
serVis(json_lda, out.dir = paste("C:/apache-tomcat-8.5.11/webapps/",name,"_",k,sep=""), open.browser = FALSE)
