# Tf-Idf 값 기준으로 dtm 크기 줄여서 new_dtm 만들기
new_dtm = dtm[,term_tfidf >= 0.15]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]
############################################
## Running LDA
############################################
#분석명, 랜덤 seed, 클러스트 개수 setup
name = "OatmealCookies"
SEED = 2017
k = 10 #클러스터 개수 세팅
#LDA 실행
lda_tm = LDA(new_dtm, control=list(seed=SEED), k)
#토픽별 핵심단어 저장하기
term_topic = terms(lda_tm, 30)
#토픽별 핵심 단어 파일로 출력하기
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
#문서별 토픽 번호 저장하기
doc_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
#문서별 토픽 확률값 계산하기
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)
#최대 확률값 찾기
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
#문서별 토픽번호 및 확률값 추출하기
doc_Prob_df$rown = doc_topic_df$rown
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","parsedData","doc_topic","maxProb"))
#문서별 토픽 번호 및 확률값 출력하기
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
write.table(id_topic, filePathName, sep=",", row.names=FALSE)
#단어별 토픽 확률값 출력하기
posterior(lda_tm)$terms
#########################################
## Make visualization
#########################################
# phi는 각 단어별 토픽에 포함될 확률값 입니다.
phi = posterior(lda_tm)$terms %>% as.matrix
# theta는 각 문서별 토픽에 포함될 확률값 입니다.
theta = posterior(lda_tm)$topics %>% as.matrix
# vocab는 전체 단어 리스트 입니다.
vocab = colnames(phi)
# 각 문서별 문서 길이를 구합니다.
doc_length = vector()
doc_topic_df=as.data.frame(doc_topic)
for( i in as.numeric(row.names(doc_topic_df))){
temp = corp[[i]]$content
doc_length = c(doc_length, nchar(temp[1]))
}
# 각 단어별 빈도수를 구합니다.
new_dtm_m = as.matrix(new_dtm)
freq_matrix = data.frame(ST = colnames(new_dtm_m),
Freq = colSums(new_dtm_m))
# 위에서 구한 값들을 파라메터 값으로 넘겨서 시각화를 하기 위한 데이터를 만들어 줍니다.
source("./Week_5/createNamJson_v2.R")
json_lda = createNamJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA가 작동 안할 때 사용
mds.method = canberraPCA
)
#Corpus 생성
corp=VCorpus(VectorSource(parsedData))
#특수문자 제거
corp = tm_map(corp, removePunctuation)
#소문자로 변경
corp = tm_map(corp, tolower)
#특정 단어 삭제
corp = tm_map(corp, removeWords, c("be","do","var","get","st","have","a","<br","/>","br"))
corp = tm_map(corp, PlainTextDocument)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
dtm = removeSparseTerms(dtm, as.numeric(0.998))
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
boxplot(term_tfidf)
new_dtm = dtm[,term_tfidf >= 0.15]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]
new_dtm
corp=VCorpus(VectorSource(parsedData))
corp = tm_map(corp, removePunctuation)
corp = tm_map(corp, tolower)
corp = tm_map(corp, removeWords, c("be","do","var","get","st","have","a","<br","/>","br"))
corp = tm_map(corp, PlainTextDocument)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
dtm = removeSparseTerms(dtm, as.numeric(0.998))
dtm
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
boxplot(term_tfidf)
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx"
,language = "en")
corp=VCorpus(VectorSource(parsedData))
corp = tm_map(corp, removePunctuation)
corp = tm_map(corp, tolower)
corp = tm_map(corp, removeWords, c("be","do","var","get","st","have","a","<br","/>","br"))
corp = tm_map(corp, PlainTextDocument)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
dtm = removeSparseTerms(dtm, as.numeric(0.998))
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
boxplot(term_tfidf)
new_dtm = dtm[,term_tfidf >= 0.15]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]
new_dtm
#분석명, 랜덤 seed, 클러스트 개수 setup
name = "OatmealCookies"
SEED = 2017
k = 10 #클러스터 개수 세팅
#LDA 실행
lda_tm = LDA(new_dtm, control=list(seed=SEED), k)
#토픽별 핵심단어 저장하기
term_topic = terms(lda_tm, 30)
#토픽별 핵심 단어 파일로 출력하기
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
#문서별 토픽 번호 저장하기
doc_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
#문서별 토픽 확률값 계산하기
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)
#최대 확률값 찾기
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
#문서별 토픽번호 및 확률값 추출하기
doc_Prob_df$rown = doc_topic_df$rown
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","parsedData","doc_topic","maxProb"))
#문서별 토픽 번호 및 확률값 출력하기
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
write.table(id_topic, filePathName, sep=",", row.names=FALSE)
#단어별 토픽 확률값 출력하기
posterior(lda_tm)$terms
#########################################
## Make visualization
#########################################
# phi는 각 단어별 토픽에 포함될 확률값 입니다.
phi = posterior(lda_tm)$terms %>% as.matrix
# theta는 각 문서별 토픽에 포함될 확률값 입니다.
theta = posterior(lda_tm)$topics %>% as.matrix
# vocab는 전체 단어 리스트 입니다.
vocab = colnames(phi)
# 각 문서별 문서 길이를 구합니다.
doc_length = vector()
doc_topic_df=as.data.frame(doc_topic)
for( i in as.numeric(row.names(doc_topic_df))){
temp = corp[[i]]$content
doc_length = c(doc_length, nchar(temp[1]))
}
# 각 단어별 빈도수를 구합니다.
new_dtm_m = as.matrix(new_dtm)
freq_matrix = data.frame(ST = colnames(new_dtm_m),
Freq = colSums(new_dtm_m))
# 위에서 구한 값들을 파라메터 값으로 넘겨서 시각화를 하기 위한 데이터를 만들어 줍니다.
source("./Week_5/createNamJson_v2.R")
json_lda = createNamJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA가 작동 안할 때 사용
mds.method = canberraPCA
)
serVis(json_lda, open.browser = T) # MAC인 경우
library(dplyr)
library(stringi)
library(tm)
library(pROC)
library(slam)
library(gmodels)
library(e1071)
library(klaR)
library(NLP4kec)
install.packages(c("pROC","gmodels","klaR","e1071"))
library(dplyr)
library(stringi)
library(tm)
library(pROC)
library(slam)
library(gmodels)
library(e1071)
library(klaR)
library(NLP4kec)
parsedData = text_parser(path = "./HomeApplication_cafe.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
library(readr)
target_val = read_csv("./training_target_val.csv")
View(target_val)
parsedData = text_parser(path = "./Blog_TrainingSet_Spam.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
target_val = read_csv("./training_target_val.csv")
parsedData = gsub(" ","  ",parsedData)
corp = VCorpus(VectorSource(parsedData))
corp = tm_map(corp, removePunctuation)
corp = tm_map(corp, tolower)
corp = tm_map(corp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))
for (j in seq(corp))
{
corp[[j]] <- gsub("lg", "엘지", corp[[j]])
corp[[j]] <- gsub("sony", "소니", corp[[j]])
}
corp = tm_map(corp, PlainTextDocument)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
dtm_removed = removeSparseTerms(dtm, as.numeric(0.98))
dtmDf = as.data.frame(as.matrix(dtm_removed))
dtmDf = dtmDf[,!duplicated(colnames(dtmDf))]
dtmDf$target = target_val$spam_yn
trainingSet = dtmDf[1:8000,] #Training 데이터 8,000개
testSet = dtmDf[8001:nrow(dtmDf),] #Test 데이터 2,012개
trainingSet$target = as.factor(trainingSet$target)
nbModel = e1071::naiveBayes(target ~ ., data = trainingSet, fL=1)
tapply(testSet$target, testSet$target, length)
nbPred = predict(nbModel, testSet[,1:(ncol(testSet)-1)])
nb_pred_result = CrossTable(table(testSet$target, nbPred), prop.chisq=FALSE)
(nb_pred_result$t[1,1] + nb_pred_result$t[2,2]) / nrow(testSet)
newData = text_parser(path = "./Blog_TestSet_Spam.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
newData = gsub(" ","  ", newData)
newCorp = VCorpus(VectorSource(newData))
newCorp = tm_map(newCorp, removePunctuation)
corp = tm_map(corp, tolower)
newCorp = tm_map(newCorp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))
for (j in seq(newCorp))
{
newCorp[[j]] <- gsub("lg", "엘지", newCorp[[j]])
newCorp[[j]] <- gsub("sony", "소니", newCorp[[j]])
}
newCorp = tm_map(newCorp, PlainTextDocument)
newDtm = DocumentTermMatrix(newCorp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
colnames(newDtm) = trimws(colnames(newDtm))
newDtm = newDtm[,nchar(colnames(newDtm)) > 1]
newDtmDf = as.data.frame(as.matrix(newDtm))
newDtmDf = newDtmDf[,!duplicated(colnames(newDtmDf))]
newDtmDf = newDtmDf[,colnames(newDtmDf) %in% colnames(trainingSet)] # 서로 같은 컬럼만 추출
add_col = trainingSet[,!colnames(trainingSet) %in% colnames(newDtmDf)] # trainingSet과 다른 컬럼 추출
add_column_nm = colnames(add_col)
forAdd = data.frame(matrix(0, ncol=length(add_column_nm), nrow=nrow(newDtmDf))) # trainingSet과 다른 컬럼을 갖는 0으로 구성된 데이터 셋 만들기
colnames(forAdd)= add_column_nm # 컬럼명 부여하기
newDtmDf = cbind(newDtmDf, forAdd) # newDtmDf에 붙여주기
nbPred_new = predict(nbModel, newDtmDf)
test_target_val = read_csv("./test_target_val.csv")
nbPred_new_df = data.frame(pred = nbPred_new, original = test_target_val$spam_yn)
nbPred_new_result = CrossTable(table(nbPred_new_df$original, nbPred_new_df$pred), prop.chisq=FALSE)
(nbPred_new_result$t[1,1] + nbPred_new_result$t[2,2]) / nrow(newData)
nbPred_new_result = CrossTable(table(nbPred_new_df$original, nbPred_new_df$pred), prop.chisq=FALSE)
(nbPred_new_result$t[1,1] + nbPred_new_result$t[2,2]) / nrow(newData)
nrow(newData)
length(newData)
(nbPred_new_result$t[1,1] + nbPred_new_result$t[2,2]) / length(newData)
test_target_val
test_target_val = read_csv("./test_target_val.csv")
nbPred_new_df = data.frame(pred = nbPred_new, original = test_target_val$spam_yn)
nbPred_new_result = CrossTable(table(nbPred_new_df$original, nbPred_new_df$pred), prop.chisq=FALSE)
(nbPred_new_result$t[1,1] + nbPred_new_result$t[2,2]) / length(newData)
parsedData = text_parser(path = "./Blog_TrainingSet_Spam.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
gc()
#형태소 분석기 실행하기
parsedData = text_parser(path = "./Blog_TrainingSet_Spam.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
# 예측 변수값 가져오기
target_val = read_csv("./training_target_val.csv")
## 단어간 스페이스 하나 더 추가하기 ##
parsedData = gsub(" ","  ",parsedData)
################################
#Text Pre-processing
################################
#Corpus 생성
corp = VCorpus(VectorSource(parsedData))
#특수문자 제거
corp = tm_map(corp, removePunctuation)
#소문자로 변경
corp = tm_map(corp, tolower)
#특정 단어 삭제
corp = tm_map(corp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))
#동의어 처리
for (j in seq(corp))
{
corp[[j]] <- gsub("lg", "엘지", corp[[j]])
corp[[j]] <- gsub("sony", "소니", corp[[j]])
}
##################################################################
#텍스트문서 형식으로 변환
corp = tm_map(corp, PlainTextDocument)
#Document Term Matrix 생성 (단어 Length는 2로 세팅)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
## 한글자 단어 제외하기 ##
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
#Sparse Terms 삭제
dtm <- removeSparseTerms(dtm, as.numeric(0.98))
#Covert to Dataframe
dtmDf = as.data.frame(as.matrix(dtm))
#중복 Column 삭제
dtmDf = dtmDf[,!duplicated(colnames(dtmDf))]
#DtmDf에 정답표 붙이기
dtmDf$target = target_val$spam_yn
#Traing Set, Test Set 만들기
trainingSet = dtmDf[1:8000,] #Training 데이터 8,000개
testSet = dtmDf[8001:nrow(dtmDf),] #Test 데이터 2,612개
#SVM 모델링
trainingSet$target = as.factor(trainingSet$target)
svmModel = svm(target ~ . , data = trainingSet, type = "C-classification",
kernel="linear", gamma=0.1, cost=1)
#Spam 문서 예측하기
svmPred =  predict(svmModel2, newdata = testSet[,1:(ncol(testSet)-1)])
#Test 데이터 확인하기
tapply(testSet$target, testSet$target, length)
#예측 결과 확인하기
svm_pred_result = CrossTable(table(testSet$target, svmPred), prop.chisq=FALSE)
#정확도 계산하기
(svm_pred_result$t[1,1] + svm_pred_result$t[2,2]) / nrow(testSet)
svmPred
svmModel = svm(target ~ . , data = trainingSet, type = "C-classification",
kernel="linear", gamma=0.1, cost=1)
gc()
library(dplyr)
library(stringi)
library(tm)
library(pROC)
library(slam)
library(gmodels)
library(e1071)
library(klaR)
library(readr)
parsedData = text_parser(path = "./Blog_TrainingSet_Spam.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
# 예측 변수값 가져오기
target_val = read_csv("./training_target_val.csv")
## 단어간 스페이스 하나 더 추가하기 ##
parsedData = gsub(" ","  ",parsedData)
################################
#Text Pre-processing
################################
#Corpus 생성
corp = VCorpus(VectorSource(parsedData))
#특수문자 제거
corp = tm_map(corp, removePunctuation)
#소문자로 변경
corp = tm_map(corp, tolower)
#특정 단어 삭제
corp = tm_map(corp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))
#동의어 처리
for (j in seq(corp))
{
corp[[j]] <- gsub("lg", "엘지", corp[[j]])
corp[[j]] <- gsub("sony", "소니", corp[[j]])
}
##################################################################
#텍스트문서 형식으로 변환
corp = tm_map(corp, PlainTextDocument)
#Document Term Matrix 생성 (단어 Length는 2로 세팅)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
## 한글자 단어 제외하기 ##
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
#Sparse Terms 삭제
dtm <- removeSparseTerms(dtm, as.numeric(0.98))
#Covert to Dataframe
dtmDf = as.data.frame(as.matrix(dtm))
#중복 Column 삭제
dtmDf = dtmDf[,!duplicated(colnames(dtmDf))]
#DtmDf에 정답표 붙이기
dtmDf$target = target_val$spam_yn
#Traing Set, Test Set 만들기
trainingSet = dtmDf[1:8000,] #Training 데이터 8,000개
testSet = dtmDf[8001:nrow(dtmDf),] #Test 데이터 2,612개
#SVM 모델링
trainingSet$target = as.factor(trainingSet$target)
svmModel = svm(target ~ . , data = trainingSet, type = "C-classification",
kernel="linear", gamma=0.1, cost=1)
#Spam 문서 예측하기
svmPred =  predict(svmModel2, newdata = testSet[,1:(ncol(testSet)-1)])
#Test 데이터 확인하기
tapply(testSet$target, testSet$target, length)
#예측 결과 확인하기
svm_pred_result = CrossTable(table(testSet$target, svmPred), prop.chisq=FALSE)
#정확도 계산하기
(svm_pred_result$t[1,1] + svm_pred_result$t[2,2]) / nrow(testSet)
library(NLP4kec)
#형태소 분석기 실행하기
parsedData = text_parser(path = "./Blog_TrainingSet_Spam.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
# 예측 변수값 가져오기
target_val = read_csv("./training_target_val.csv")
## 단어간 스페이스 하나 더 추가하기 ##
parsedData = gsub(" ","  ",parsedData)
################################
#Text Pre-processing
################################
#Corpus 생성
corp = VCorpus(VectorSource(parsedData))
#특수문자 제거
corp = tm_map(corp, removePunctuation)
#소문자로 변경
corp = tm_map(corp, tolower)
#특정 단어 삭제
corp = tm_map(corp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))
#동의어 처리
for (j in seq(corp))
{
corp[[j]] <- gsub("lg", "엘지", corp[[j]])
corp[[j]] <- gsub("sony", "소니", corp[[j]])
}
##################################################################
#텍스트문서 형식으로 변환
corp = tm_map(corp, PlainTextDocument)
#Document Term Matrix 생성 (단어 Length는 2로 세팅)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
## 한글자 단어 제외하기 ##
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
#Sparse Terms 삭제
dtm <- removeSparseTerms(dtm, as.numeric(0.98))
#Covert to Dataframe
dtmDf = as.data.frame(as.matrix(dtm))
#중복 Column 삭제
dtmDf = dtmDf[,!duplicated(colnames(dtmDf))]
#DtmDf에 정답표 붙이기
dtmDf$target = target_val$spam_yn
#Traing Set, Test Set 만들기
trainingSet = dtmDf[1:8000,] #Training 데이터 8,000개
testSet = dtmDf[8001:nrow(dtmDf),] #Test 데이터 2,612개
#SVM 모델링
trainingSet$target = as.factor(trainingSet$target)
svmModel = svm(target ~ . , data = trainingSet, type = "C-classification",
kernel="linear", gamma=0.1, cost=1)
#Spam 문서 예측하기
svmPred =  predict(svmModel2, newdata = testSet[,1:(ncol(testSet)-1)])
#Test 데이터 확인하기
tapply(testSet$target, testSet$target, length)
#예측 결과 확인하기
svm_pred_result = CrossTable(table(testSet$target, svmPred), prop.chisq=FALSE)
#정확도 계산하기
(svm_pred_result$t[1,1] + svm_pred_result$t[2,2]) / nrow(testSet)
detach("package:NLP4kec", unload=TRUE)
gc()
detach("package:rJava", unload=TRUE)
gc()
svmModel = svm(target ~ . , data = trainingSet, type = "C-classification",
kernel="linear", gamma=0.1, cost=1)
#Spam 문서 예측하기
svmPred =  predict(svmModel2, newdata = testSet[,1:(ncol(testSet)-1)])
#Test 데이터 확인하기
tapply(testSet$target, testSet$target, length)
#예측 결과 확인하기
svm_pred_result = CrossTable(table(testSet$target, svmPred), prop.chisq=FALSE)
#정확도 계산하기
(svm_pred_result$t[1,1] + svm_pred_result$t[2,2]) / nrow(testSet)
svmModel
svmPred =  predict(svmModel, newdata = testSet[,1:(ncol(testSet)-1)])
svm_pred_result = CrossTable(table(testSet$target, svmPred), prop.chisq=FALSE)
(svm_pred_result$t[1,1] + svm_pred_result$t[2,2]) / nrow(testSet)
newData = text_parser(path = "./Blog_TestSet_Spam.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
library(NLP4kec)
newData = text_parser(path = "./Blog_TestSet_Spam.xlsx"
,language = "ko"
,korDicPath = "./dictionary.txt")
newData = gsub(" ","  ", newData)
newCorp = VCorpus(VectorSource(newData))
newCorp = tm_map(newCorp, removePunctuation)
corp = tm_map(corp, tolower)
newCorp = tm_map(newCorp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))
for (j in seq(newCorp))
{
newCorp[[j]] <- gsub("lg", "엘지", newCorp[[j]])
newCorp[[j]] <- gsub("sony", "소니", newCorp[[j]])
}
#텍스트문서 형식으로 변환
newCorp = tm_map(newCorp, PlainTextDocument)
newDtm = DocumentTermMatrix(newCorp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
## 한글자 단어 제외하기 ##
colnames(newDtm) = trimws(colnames(newDtm))
newDtm = newDtm[,nchar(colnames(newDtm)) > 1]
newDtmDf = as.data.frame(as.matrix(newDtm))
newDtmDf = newDtmDf[,colnames(newDtmDf) %in% colnames(trainingSet)] # 서로 같은 컬럼만 추출
add_col = trainingSet[,!colnames(trainingSet) %in% colnames(newDtmDf)] # trainingSet과 다른 컬럼 추출
add_column_nm = colnames(add_col)
forAdd = data.frame(matrix(0, ncol=length(add_column_nm), nrow=nrow(newDtmDf))) # trainingSet과 다른 컬럼을 갖는 0으로 구성된 데이터 셋 만들기
colnames(forAdd)= add_column_nm # 컬럼명 부여하기
newDtmDf = cbind(newDtmDf, forAdd) # newDtmDf에 붙여주기
svmPred_new = predict(svmModel, newDtmDf)
test_target_val = read_csv("./test_target_val.csv")
svmPred_new_df = data.frame(pred = svmPred_new, original = test_target_val$spam_yn)
svmPred_new_result = CrossTable(table(svmPred_new_df$original, svmPred_new_df$pred), prop.chisq=FALSE)
(svmPred_new_result$t[1,1] + svmPred_new_result$t[2,2]) / nrow(newData)
(svmPred_new_result$t[1,1] + svmPred_new_result$t[2,2]) / length(newData)
svmPred_new_result = CrossTable(table(svmPred_new_df$original, svmPred_new_df$pred), prop.chisq=FALSE, prop.r = F)
svmPred_new_result = CrossTable(table(svmPred_new_df$original, svmPred_new_df$pred), prop.chisq=FALSE)
(svmPred_new_result$t[1,1] + svmPred_new_result$t[2,2]) / length(newData)
