{
    "collab_server" : "",
    "contents" : "install.packages(\"tm\") #텍스트 마이닝을 위한 패키지\ninstall.packages(\"slam\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"readr\") #파일을 읽어오기 위한 패키지\n\nlibrary(tm)\nlibrary(slam)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(NLP4kec)\n\n#형태소 분석기 실행하기\nparsedData = text_parser(path = \"/Users/kimnamyoun/TextConvert4TM/input/HomeApplication_cafe.xlsx\"\n                         ,language = \"ko\"\n                         ,korDicPath = \"./dictionary.txt\")\n\n\n## 단어간 스페이스 하나 더 추가하기 ##\nparsedData = gsub(\" \",\"  \",parsedData)\n\n##################################################################\n#Text Pre-processing\n##################################################################\n\n#Corpus 생성\ncorp = VCorpus(VectorSource(parsedData))\n\n#특수문자 제거\ncorp = tm_map(corp, removePunctuation)\n\n#숫자 삭제\ncorp = tm_map(corp, removeNumbers)\n\n#소문자로 변경\ncorp = tm_map(corp, tolower)\n\n#특정 단어 삭제\ncorp = tm_map(corp, removeWords, c(\"있다\", \"하다\",\"그렇다\",\"되다\",\"같다\",\"가다\",\"없다\",\"보다\",\"정도\"))\n\n#동의어 처리\nfor (j in seq(corp))\n{\n  corp[[j]] <- gsub(\"lg\", \"엘지\", corp[[j]])\n  corp[[j]] <- gsub(\"samsung\", \"삼성\", corp[[j]])\n}\n##################################################################\n\n#텍스트문서 형식으로 변환\ncorp = tm_map(corp, PlainTextDocument)\n\n#Document Term Matrix 생성 (단어 Length는 2로 세팅)\ndtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))\n\n## 단어 양옆 스페이스 제거 및 한글자 단어 제외하기\ncolnames(dtm) = trimws(colnames(dtm))\ndtm = dtm[,nchar(colnames(dtm)) > 1]\n\n#Term Document Matirx 생성 (DTM에서 행과 열만 바뀐 matrix)\ntdm = TermDocumentMatrix(corp, control=list(removeNumbers=TRUE, wordLengths=c(2,Inf)))\n\n#Sparse Terms 삭제 (값이 작아질 수록 term수가 줄어든다.)\ndtm = removeSparseTerms(dtm, as.numeric(0.99))\n\n#단어 발생 빈도 구하기\nfreq = colSums(as.matrix(dtm))\n\n#DTM을 데이터 프레임 형식으로 저장하기\ndtm_df = as.data.frame(as.matrix(dtm))\n\n#DTM을 CSV로 추출해서 확인해보기\nwrite_excel_csv(dtm_df, \"./dtm.csv\")\n\n#단어 개수 구하기\nlength(freq)\n\n#내림차순으로 단어 10개, sorting 하기\nfreq[head(order(-freq), 5)]\n\n#오름차순으로 단어 10개 sorting 하기\nfreq[head(order(freq), 10)]\n\n#특정 빈도 사이값을 갖는 단어 구하기 (20보다 크고 341보다 작은 단어)\nfindFreqTerms(dtm, lowfreq = 20, highfreq = 341)\n\n#단어 빈도 시각화\nwordDf = data.frame(word=names(freq), freq=freq)\nlibrary(ggplot2)\n\n#맥북 사용자는 폰트 import하기\ninstall.packages(\"extrafont\")\nlibrary(extrafont)\n#font_import()\nloadfonts(device=\"postscript\")\n\n#단어 빈도수 바차트로 보여주기\nggplot(wordDf, aes(x=word, y=freq)) + geom_bar(stat = \"identity\")\n\n#단어 10개만 바차트로 보여주기\nggplot(head(wordDf,10), aes(x=word, y=freq)) + geom_bar(stat = \"identity\")\n\n#상위 20개 단어만 바차트로 보여주기\nggplot(head(arrange(wordDf,-freq),20), aes(x=reorder(word,-freq), y=freq)) + geom_bar(stat = \"identity\")\n\n\n#Word Cloud 그리기\ninstall.packages(\"wordcloud\")\nlibrary(wordcloud)\npal = brewer.pal(n = 3, name = \"Set2\") # n:사용할 색깔 수, name:색깔 조합 이름\n\nwordcloud(wordDf$word # 단어\n          , wordDf$freq # 빈도수\n          , min.freq = 5 # 표현할 단어의 최소 빈도수\n          , colors = pal # 위에서 만든 팔레트 정보 입력\n          , rot.per = 0 # 단어의 회전 각도\n          , random.order = F # 단어의 노출 순서 랜덤 여부 결정\n          , scale = c(3,1)) # scale값에서 앞에 값이 커야 빈도수가 큰 단어 사이즈가 커야함\n\n\n#treeMap 그리기\ninstall.packages(\"treemap\")\nlibrary(treemap)\ntreemap(wordDf # 대상 데이터 설정\n        ,title = \"Word Tree Map\"\n        ,index = c(\"word\") # 박스 안에 들어갈 변수 설정\n        ,vSize = \"freq\"  # 박스 크기 기준\n        ,fontsize.labels = 12 # 폰트 크기 설정\n        ,palette=pal # 위에서 만든 팔레트 정보 입력\n        ,border.col = \"white\") # 경계선 색깔 설정\n",
    "created" : 1497757401399.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1689656058",
    "id" : "4C6C6A52",
    "lastKnownWriteTime" : 1497757681,
    "last_content_update" : 1497757681220,
    "path" : "~/GitHub/RWork/FastCampus/text_handling.R",
    "project_path" : "text_handling.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}