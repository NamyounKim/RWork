---
title: "topic_clustering"
author: "Namyoun"
date: "2017년 5월 9일"
output: html_document
---

```{r}
library(topicmodels)
library(LDAvis)
library(servr)
library(readr)
library(tm)
library(slam)
library(dplyr)

#분석 결과 가져오기
parsedData =read_csv("c:/TextConvert4TM_v1.0/output/out_HomeApplication_cafe.csv")

## 단어간 스페이스 하나 더 추가하기 ##
parsedDataRe = parsedData
parsedDataRe$pContent = gsub(" ","  ",parsedDataRe$parsedContent)

##################################################################
#Text Pre-processing
##################################################################
#Corpus 생성
corp=VCorpus(VectorSource(parsedDataRe$pContent))

#특수문자 제거
corp = tm_map(corp, removePunctuation)

#특정 단어 삭제
corp = tm_map(corp, removeWords, c("있다", "하다","그렇다","되다","같다","가다","없다","보다","정도","000원","030원","주세요","어떻다"))

#동의어 처리
for (j in seq(corp))
{
  corp[[j]] = gsub("lg", "엘지", corp[[j]])
  corp[[j]] = gsub("samsung", "삼성", corp[[j]])
}
##################################################################

#텍스트문서 형식으로 변환
corp = tm_map(corp, PlainTextDocument)

#Document Term Matrix 생성 (단어 Length는 2로 세팅)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))

## 한글자 단어 제외하기 ##
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]

#Sparse Terms 삭제
dtm = removeSparseTerms(dtm, as.numeric(0.997))

## LDA 할 때 DTM 크기 조절
#단어별 Tf-Idf 값 구하기
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))

#박스그래프로 분포 확인
boxplot(term_tfidf)

# Tf-Idf 값 기준으로 dtm 크기 줄여서 new_dtm 만들기
new_dtm = dtm[,term_tfidf >= 0.1]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]

############################################
## Running LDA
############################################
#분석명, 랜덤 seed, 클러스트 개수 setup
name = "HomeApplication"
SEED = 2017
k = 10 #클러스터 개수 세팅

#LDA 실행
lda_tm = LDA(new_dtm, control=list(seed=SEED), k)

#토픽별 핵심단어 저장하기
term_topic = terms(lda_tm, 30)
term_topic

#토픽별 핵심 단어 파일로 출력하기
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
write.table(term_topic, filePathName, sep=",", row.names=FALSE)

#문서별 토픽 번호 저장하기
doc_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))

#문서별 토픽 확률값 계산하기
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)

#최대 확률값 찾기
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)

#문서별 토픽번호 및 확률값 추출하기
doc_Prob_df$rown = doc_topic_df$rown
parsedData$rown = as.numeric(row.names(parsedData))
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","id","doc_topic","maxProb"))

#문서별 토픽 번호 및 확률값 출력하기
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
write.table(id_topic, filePathName, sep=",", row.names=FALSE)


```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
