---
title: "중앙경제HR교육원 - text mining"
output:
  html_document: 
        toc: true
        number_sections: true
        theme: cosmo
        df_print: paged
        rows.print: 8
        pages.print: 100
  html_notebook: default
---

<h4> Background </h4>
<p> 텍스트 분석에 대한 R notebook입니다. </p>
<br>
<h4> Change History </h4>
* ...
<br>

#차트/그래프 테마작업
ggplot 차트/폰트가 예쁘게 나오도록 하는 테마작업
```{r, message=FALSE}
suppressPackageStartupMessages({
library(ggplot2)
library(ggthemes)
})

#base_family = "malgun" for window
theme_gogamza<- function(base_size = 9, base_family = "NanumGothic"){
     (theme_foundation(base_size = base_size, base_family = base_family) +
        theme(line = element_line(colour = "black"), rect = element_rect(fill = ggthemes_data$fivethirtyeight["ltgray"],
            linetype = 0, colour = NA), text = element_text(colour = ggthemes_data$fivethirtyeight["dkgray"]),
            axis.title = element_text(), axis.text = element_text(),
            axis.ticks = element_blank(), axis.line = element_blank(),
            legend.background = element_rect(), legend.position = "bottom",
            legend.direction = "horizontal", legend.box = "vertical",
            panel.grid = element_line(colour = NULL), panel.grid.major = element_line(colour = ggthemes_data$fivethirtyeight["medgray"]),
            panel.grid.minor = element_blank(), plot.title = element_text(hjust = 0,
                size = rel(1.5), face = "bold"), plot.margin = grid::unit(c(1,
                1, 0.5, 0.5), "lines"), strip.background = element_rect(), panel.margin.x=NULL, panel.margin.y=NULL))
}

#ggplot2 메인 테마로 설정
theme_set(theme_gogamza())

#기본 테마에서 한글 폰트 정의를 하고 싶다면 아래 코드만 실행한다. 
theme_set(theme_gray(base_family='NanumGothic'))
```

#KoNLP 및 관련 패키지 로딩
사전은 NIA 사전을 사용
```{r, warning=FALSE, message=FALSE}
# for 한글 형태소/토큰 추출; RWeka 먼저 실행하면 윈도우에서 문제 있기도 하다네요.
library(KoNLP) 
library("rJava", "RWeka")
#library(data.table)
library(tm)
library(dplyr)
library(tidytext)
#library(stringr)
library(tidyr)
library(igraph)
library(ggraph)
library(RColorBrewer) 
library(wordcloud)
#library(wordcloud2)
library(widyr) #for word correlation
vignette("KoNLP-API") 
#library(highcharter)
#useSystemDic()
#useSejongDic()
useNIADic() #NIA 사전 사용
```

#엑셀 로딩 및 data pre-processing
```{r, warning=FALSE, message=FALSE }
#메모리 4G 할당
options(java.parameters = "-Xmx4g" )
library("XLConnect")
setwd("/Users/apple/Google 드라이브/HR Science School/ R code data/텍스트/")
comment <- readWorksheetFromFile("덕혜옹주리뷰.xlsx", sheet = 1)

#check the feedback histogram
qplot(comment$영화평점, geom="histogram") 
ggplot(data=comment, aes(comment$영화평점)) + 
  geom_histogram(aes(y =..density..), 
                 breaks=seq(20, 50, by = 2), 
                 col="red", 
                 fill="green", 
                 alpha = .2) + 
  geom_density(col=2) + 
  labs(title="Histogram for Comment") +
  labs(x="Comment Point", y="Count")

comment <- comment %>%
  mutate(평점.등급 = ifelse(영화평점 >= 10, "Good",
                ifelse(영화평점 >= 5, "So So", "Bad")))
table(comment$평점.등급)
comment$value <- comment$리뷰내용

```

#해당 문서집단의 Token/Keyword 특성 분석
##Corpus 생성 및 전처리
```{r, warning=FALSE, message=FALSE }

comment$value <- gsub("\n", " ", comment$value)
comment$value <- gsub("\r", " ", comment$value)
comment$value <- gsub("'", "", comment$value)  # remove apostrophes
comment$value <- gsub("[[:punct:]]", " ", comment$value)  # replace punctuation with space
comment$value <- gsub("[[:cntrl:]]", " ", comment$value)  # replace control characters with space
comment$value <- tolower(comment$value)  # force to lowercase

#각 레코드별로 명사 추출하여 별도 칼럼 생성
for (i in 1:dim(comment)[1]) {
    if (!is.na(comment$value[i]) & nchar(comment$value[i]) > 10) {
      l <- extractNoun(comment$value[i])
      l <- l[lapply(l, nchar) > 1]
      comment$new.comment[i] <- paste(l, collapse=' ')
    } else {
      comment$new.comment[i] <- ""  
    }
}

#word processing 두 단어로 인식된 "문제 해결"을 "문제해결" 한 단어로 처리
comment$new.comment <- gsub("문제 해결","문제해결", comment$new.comment)
comment$new.comment <- gsub(" 하기","", comment$new.comment)
comment$new.comment <- gsub(" 하려","", comment$new.comment)
comment$new.comment <- gsub(" 하고","", comment$new.comment)
our.stopword <- c("빼고싶은","단어들을","여기에적는다")

#make a signle long character vector (large string) from Corpus
Corpus <- paste(comment[which(comment$new.comment != "") ,"new.comment"], collapse = " ")
```


##단어(Keyword) 추출 및 wordcloud & 키워드 빈도 검토
```{r, warning=FALSE, message=FALSE }
# how mancy characters
nchar(Corpus)

# make a vector comprised of token(noun)
wordsvec <- unlist(strsplit(Corpus, split=" "))
length(wordsvec)

#2글자 이상인 키워드만 (중복 규칙)
wordsvec <- Filter(function(x){nchar(x)>=2}, wordsvec)
wordscount <- table(wordsvec)
wordscount <- as.data.frame(wordscount)

#ggplot bar chart
wordscount %>% 
  arrange(desc(Freq)) %>%
  top_n(20) %>% 
  ungroup %>%
  ggplot(aes(x = reorder(wordsvec, Freq), y = Freq)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Term Frequency") +
  coord_flip()


#wordcloud
#family = "malgun" for window
pal <- brewer.pal(8,"Dark2")
Sys.setenv(LANG = "en")
par(family="AppleGothic")
wordcloud(words=wordscount$wordsvec, freq=wordscount$Freq, scale=c(4,.5),min.freq=10,
          random.order=T,rot.per=.1,colors=pal)

wordscount %>% 
  arrange(desc(Freq)) %>%
  top_n(100)
```


##bi-gram 생성
```{r}
#윈도우에서 bi-gram을 추출할 때 unnest_token 함수를 사용하는 경우 한글 encoding이 깨지는 경우 아래 코드를 사용
#change 2 to 3 to extract tri-gram 세개의 음절 뽑아내려면 ngrams 함수에서 2를 3으로 바꿈.
bigrams <- comment
bigrams$bigram <- NA
for (i in 1:dim(comment)[1]) {
  if (!is.na(bigrams$new.comment[i]) & nchar(bigrams$new.comment[i]) > 2) {  
    bi.list <- unlist(lapply(ngrams(words(bigrams$new.comment[i]), 2), paste, collapse = " "), use.names = FALSE)
    if (!is.null(bi.list)) {
      bi.list <- paste(bi.list, collapse=',')
#      print(bi.list)
      bigrams$bigram[i] <- bi.list
    }
  }
}
bigrams <- bigrams %>%
  unnest(bigram = strsplit(bigram, ","))
bigrams <- bigrams[!(is.na(bigrams$bigram)),]


#맥에서는 아래 코드 수행
#bigrams <- comment[which(comment$new.comment != ""), ] %>%
#  unnest_tokens(bigram, new.comment, token = "ngrams", n = 2)

##bi-gram new code - filtering out 본인 평가
#본인 평가만 삭제하고 싶은 경우 아래 코드 실행
#bigrams <- bigrams %>% filter(!grepl('본인', attribute))
#unique(bigrams$attribute)

bigrams_count <- bigrams %>% 
  count(bigram, sort = TRUE)

bigrams_count %>%
  top_n(100)

bigrams_count %>% 
  arrange(desc(n)) %>%
#  filter (n<50) %>%
  top_n(20) %>% 
  ungroup %>%
  ggplot(aes(x = reorder(bigram, n), y = n)) +
#  ggplot(aes(x = bigram, y = n)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Term Frequency") +
  coord_flip()

#wordcloud
#use smaller scale "cloud ould not be fit on page. It will not be plotted.""
pal <- brewer.pal(8,"Dark2")
Sys.setenv(LANG = "en")
par(family="AppleGothic")
wordcloud(words=bigrams_count$bigram, freq=bigrams_count$n, scale=c(4,0.5),min.freq=10,
          random.order=T,rot.per=.1,colors=pal)

#head(bigrams_count, 30)
```


##Keyword co-ocurrences (graph)
두 개의 연속된 keyword를 사용해서 graph 그리기
```{r}
#c_bigrams <- comment[which(comment$new.comment != ""), ] %>%
#  unnest_tokens(bigram, new.comment, token = "ngrams", n = 2)

bigrams %>%
  count(bigram, sort = TRUE) %>%
  top_n(100)

bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(nchar(word1)>1) %>%
  filter(nchar(word2)>1)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts %>%
  top_n(100)

bigram_graph <- bigram_counts %>%
  arrange(desc(n)) %>%
  top_n(100) %>% 
# filter(n > 30) %>%
  graph_from_data_frame()

bigram_graph 

set.seed(2017)
#graph type 1
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
set_graph_style(family = "AppleGothic", face = "plain", size = 10, text_size = 9, text_colour = "black")
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

#graph type 2
bigram_counts %>%
  filter(n >= 10) %>%
#  filter(word1 == "의견") %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4", arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()


#graph type 2 with specific words
bigram_counts %>%
  filter(word1 == "연기") %>%
  top_n(20) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4", arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()

#graph type 3 (without direction)
bigram_counts %>%
#  filter(n >= 50) %>%
#  filter(n <= 200) %>%
  top_n(100) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "darkred") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```


##word pair count
단어의 연속성을 고려하지 않고 동일한 평가의견에서 함께 사용된 단어를 가지고 함께 사용 빈도를 계산/시각화
```{r, warning=FALSE, message=FALSE}

#aading row ID
comment$ID <- seq.int(nrow(comment))

comment.keywords <- comment %>%
  unnest_tokens(keyword, new.comment) %>%
  filter(!keyword %in% stop_words$word)

# count words co-occuring within the same feedback
word_pairs <- comment.keywords %>%
  pairwise_count(keyword, ID, sort = TRUE)

word_pairs %>%
  top_n(100)

word_pairs %>%
  filter(item1 == "연기") %>%
  top_n(25) %>% 
#  filter(n <= 300) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "darkred") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()

word_pairs %>%
  top_n(70) %>% 
#  filter(n <= 300) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "darkred") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```


##word pair correlation
단어의 연속성을 고려하지 않고 동일한 평가의견에서 함께 사용된 단어를 가지고 두 단어간 상관관계를 계산/시각화
The pairwise_cor() function in widyr lets us find the phi coefficient between words based on how often they appear in the same feedback.
```{r, warning=FALSE, message=FALSE}
#correlation이 1이면 표절, 카피&페이스트, 자기표절...??
word_cors <- comment.keywords %>%
  group_by(keyword) %>%
  filter(n() >= 30) %>%
  pairwise_cor(keyword, ID, sort = TRUE)

word_cors %>%
  top_n(100)

word_cors %>%
  filter(item1 == "패기")

#word correlation bar chart for several keywords
theme_set(theme_gogamza())
word_cors %>%
  filter(item1 %in% c("연기", "역사", "감동")) %>%
  group_by(item1) %>%
  top_n(10) %>%
#  arrange(desc(correlation)) %>%
#  ungroup() %>%
#  mutate(item2 = reorder(item2, correlation)) %>%
#  ggplot(aes(x = reorder(bigram, tf_idf), y = tf_idf, fill = 종합등급.2016)) +
  ggplot(aes(x= reorder(item2, correlation), y=correlation)) +
  geom_bar(stat = "identity") +
  labs(x = NULL, y = "correlation") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

#word correlation graph
set_graph_style(family = "AppleGothic", face = "plain", size = 10, text_size = 9, text_colour = "black")
word_cors %>%
  filter(correlation > .05) %>%
  top_n(100) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

#Compare Token Characteristics - Segment Setting
##여기를 바꾸세요
비교할 세그먼트를 class에 대입하면 됨
연도별로 비교하려면 종합등급.2016 대신 Year를
평가자별로 비교하려면 evaluator를 입력
```{r, warning=FALSE, message=FALSE }
#setting the class variable
comment$class <- comment$평점.등급
comment$class[comment$class==""] <- "NA"
comment$class[is.na(comment$class)] <- "NA"
comment$class[comment$class==" "] <- "NA"

bigrams$class <- bigrams$평점.등급
bigrams$class[bigrams$class==""] <- "NA"
bigrams$class[is.na(bigrams$class)] <- "NA"
bigrams$class[bigrams$class==" "] <- "NA"
```

##주어진 직원 세그먼트에 대하여 토큰(uni-gram; 한단어)의 특성 비교
```{r, warning=FALSE, message=FALSE }
comment.word <- comment %>%
  unnest_tokens(word, new.comment) %>%
  count(class, word, sort=TRUE) %>%
  ungroup()

total.word <- comment.word %>%
  group_by(class) %>%
  summarise(total = sum(n))
  
comment.word <- left_join(comment.word, total.word)

comment.word <- comment.word %>%
  group_by(class) %>%
  mutate("tf" = n/total) %>%
  arrange(tf) %>%
  mutate(tf_rank = row_number())

theme_set(theme_gogamza())

#by frequency
comment.word %>% 
  group_by(class) %>% 
  arrange(desc(tf)) %>%
  top_n(n=12, wt=tf_rank) %>% 
  ungroup %>%
  ggplot(aes(x = reorder(word, tf), y = tf, fill = class)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Term Frequency") +
  facet_wrap(~class, ncol = 3, scales = "free") +
  coord_flip()


#by tf-idf
comment.word.tf_idf <- comment.word %>%
  bind_tf_idf(word, class, n)

comment.word.tf_idf <- comment.word.tf_idf %>%
  group_by(class) %>%
  arrange(tf_idf) %>%
  mutate(tf_idf_rank = row_number())

comment.word.tf_idf %>% 
  group_by(class) %>% 
  arrange(desc(tf_idf)) %>%
  top_n(n=12, wt=tf_idf_rank) %>% 
  ungroup %>%
  ggplot(aes(x = reorder(word, tf_idf), y = tf_idf, fill = class)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~class, ncol = 3, scales = "free") +
  coord_flip()
```



##bi-gram tf-idf against segmentation (given class)
class별로 tf, tf-idf 기준으로 bi-gram을 비교한 내용
```{r, warning=FALSE, message=FALSE }
#separate the words
bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigram_counts <- bigrams_separated %>% 
  count(word1, word2, sort = TRUE)
bigram_counts %>% 
  top_n(100)

# unite bigrams
bigrams_united <- bigrams_separated %>%
  unite(bigram, word1, word2, sep = " ")

#creating bigram tf_idf
bigram_tf_idf <- bigrams_united %>%
  count(class, bigram) %>%
  bind_tf_idf(bigram, class, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  top_n(100)

#by frequency
bigram_tf_idf <- bigram_tf_idf %>%
  group_by(class) %>%
  arrange(tf) %>% 
  mutate(tf_rank = row_number())

theme_set(theme_gogamza())
bigram_tf_idf %>% 
  group_by(class) %>% 
  arrange(desc(tf)) %>%
  top_n(n=12, wt=tf_rank) %>% 
  ungroup %>%
  ggplot(aes(x = reorder(bigram, n), y = n, fill = class)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "term count") +
  facet_wrap(~class, ncol = 3, scales = "free") +
  coord_flip() 


#by tf-idf
bigram_tf_idf <- bigram_tf_idf %>%
  group_by(class) %>%
  arrange(tf_idf) %>% 
  mutate(tf_idf_rank = row_number())

theme_set(theme_gogamza())

bigram_tf_idf %>% 
  group_by(class) %>% 
  top_n(12, wt=tf_idf_rank) %>% 
  ungroup %>%
  ggplot(aes(x = reorder(bigram, tf_idf), y = tf_idf, fill = class)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~class, ncol = 3, scales = "free") +
  coord_flip() 

#comment.output <- comment.df
#comment.output$name <- comment.output$word
#write.table(comment.output,"~/Google 드라이브/R/performemce text/SKHynix/bigram_per_grade.170628.csv", sep=",",row.names=FALSE)
```

