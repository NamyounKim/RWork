{
    "collab_server" : "",
    "contents" : "rm(list=ls())\ngc()\n\nstart.time <- Sys.time()\nsource(\"/home/ruser/TextPrism/RSource/ML_functions.R\")\nsource(\"/home/ruser/TextPrism/RSource/createNamJson.R\")\nsource(\"/home/ruser/TextPrism/RSource/config_LDA.R\")\nsource(\"/home/ruser/TextPrism/RSource/network_v1.1.R\")\n\n\n#package check & install & load\nlibraryList <- c(\"dplyr\",\"stringi\",\"tm\",\"caret\",\"reshape\",\"lsa\",\"RODBC\",\"RODBCext\",\"topicmodels\",\"servr\",\"LDAvis\")\n\nfor(lib in libraryList){\n  package.checking <- find.package(lib,quiet=TRUE)\n  if(length(package.checking) == 0){\n    install.packages(lib)\n  }\n}\nrequire(dplyr)\nrequire(stringi)\nrequire(tm)\nrequire(reshape)\nrequire(slam)\nrequire(SnowballC)\nrequire(topicmodels)\nrequire(RODBC)\nrequire(RODBCext)\nrequire(servr)\nrequire(LDAvis)\nrequire(doParallel)\n\nregisterDoParallel()\n\nsparseRe <- gsub(\"0.\",\"_\",sparse)\nprint(visual)\nprint(name)\nprint(paste(\"Sparse Term Ratio :\",sparse))\nprint(paste(\"Number of Cluster:\",k))\nprint(Sys.time())\n\n\n\n############################################\n## Read in TM results\n############################################\n\n#DB Connection\nprint(\"Connect DB\")\n#conn <- odbcConnect('ccaMart',uid='trendtracker',pwd='#tt1234')\nconn <- odbcConnect('smartSMA_Development_New',uid='trendtracker',pwd='#tt1234')\n#conn <- odbcConnect('Alibaba',uid='trendtracker',pwd='#tt1234')\nprint(\"Loading Data from DB\")\n\ntm<-sqlQuery(conn,inputQuery)\nodbcClose(conn)\n\n#tm <- read.csv(file=\"/home/ruser/TextPrism/input/out_G5_youtube_dbinsert.csv\")\ntm <- subset(tm,as.integer(tm$ranking)>0)\n\ndisuse.role <-c(\"hash\",\"NNBJKS\",\"SN\",\"MAJ\",\"UNKNOWN\",\"MAG\",\"VCPEC\",\"url\",\"SL\",\"$$\")\n#disuse.role <-c(\"$$\")\n\n#disuse.term <- c(\"하다\",\"되다\",\"있다\",\"같다\",\"좋다\",\"자형\",\"point\",\"네요\",\"the\",\"one\",\"pc\",\"nc\",\"of\",\"to\",\"for\",\"in\",\"ess\",\"포항\",\"청약\",\"무주택자\",\"vs\",\"or\"\n#                ,\"mm\",\"입니다\",\"gg\",\"으십니다\",\"셔서\",\"라는\",\"모바\",\"비스\",\"mr\",\"xl\",\"이야\",\"eq\",\"se\",\"rt\",\"bt\",\"be\",\"ea\",\"on\",\"from\",\"cv\",\"nbsp\",\"ed\",\"eb\",\"ac\",\"bc\")\ndisuse.term <- c(\"nm\",\"go\",\"year\",\".no\",\"use\",\"autoguide.com\",\"dx\",\"tt\",\"news.more\",\"rm\",\"$$\",\"autocar\",\"eos\",\"nx\",\"mall\",\"st.\",\"@media\",\"div_*\",\"ps\",\"bloomberg\",\"20ps\")\n\ntm$keyword <- gsub(\" \", \"#\", tm$keyword)\ntm <- tm[!(tm$keyword %in% disuse.term),]\ntm <- tm[!(tm$role %in% disuse.role),]\ntm <- tm[length(tm$keyword)>1,]\n\n\n## TM results into document keywords matrices\nprint(\"Make DTM\")\ntmKeyword <- fn_tm_keys(tm)\nprint(paste(\"Total Document :\",nrow(tmKeyword)))\n\n####################################\n## Manual Spam Check\n####################################\nif(MSC){\n  spamDocId <- read.table(file=\"./TextPrism/spamDocId.txt\", header=TRUE)\n  spamCheck <- tmKeyword$crawl_data_id %in% spamDocId$spamDocId\n  tmKeyword <- tmKeyword[!spamCheck,]\n}\n\n##Duplication Check\ndupCheck <- duplicated(tmKeyword[,2])\ntmKeyword <- tmKeyword[!dupCheck,]\n\nprint(paste(\"Target Document :\",nrow(tmKeyword)))\ncorp<-Corpus(DataframeSource(tmKeyword))\ndtm<-DocumentTermMatrix(corp, control=list(removeNumbers=TRUE, wordLengths=c(2,Inf)))\ndtm <- removeSparseTerms(dtm, as.numeric(sparse))\n\n##Remove low tf-idf col and row\nterm_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))\nnew_dtm <-dtm[,term_tfidf >= 0.1]\nnew_dtm <-new_dtm[row_sums(new_dtm)>0,]\n\n############################################\n## Running LDA\n############################################\nprint(\"Start LDA\")\nSEED <-2010\n\nlda_tm <- LDA(new_dtm, control=list(seed=SEED), k=as.numeric(k))\n\ndoc_topic <- topics(lda_tm,1)\nterm_topic <- terms(lda_tm, numTermByTopic)\n\n\nwrite.table(term_topic, paste(\"/home/ruser/TextPrism/output/\",name,sparseRe,\"_\",k,\"_LDA_Result.csv\",sep=\"\"),sep=\",\", row.names=FALSE)\n\n\n#Doc_topic result making\ndoc_topic_df <- as.data.frame(doc_topic)\ndoc_topic_df$rown <- as.numeric(row.names(doc_topic_df))\ntmKeyword$rown <- as.numeric(row.names(tmKeyword))\n\n#doc prob make\ndocProb <- posterior(lda_tm)$topics %>% as.matrix\ndocProb_df <- as.data.frame(docProb)\ndocProb_df$rown <- as.numeric(row.names(docProb_df))\nmax<-NULL\nfor(i in 1:nrow(docProb_df)){\n\tmax<-rbind(max,max(docProb[i,]))\n}\ndocProb_df$maxProb<-max\n\nid_topic <- merge(doc_topic_df, docProb_df, by=\"rown\")\nid_topic <- merge(id_topic, tmKeyword, by=\"rown\")\nid_topic <- subset(id_topic,select=c(\"rown\",\"doc_topic\",\"crawl_data_id\",\"maxProb\"))\n\nwrite.table(id_topic, paste(\"/home/ruser/TextPrism/output/\",name,sparseRe,\"_\",k,\"_raw\",\"_LDA_Result.csv\",sep=\"\"),sep=\",\", row.names=FALSE)\n\noutPutForQV <- fn_LDA_Result_for_QV(term_topic)\nwrite.table(outPutForQV, paste(\"/home/ruser/TextPrism/output/\",name,sparseRe,\"_\",k,\"_QV\",\"_LDA_Result.csv\",sep=\"\"),sep=\",\", row.names=FALSE)\n\n#########################################\n## Make visualization\n#########################################\nif(visual){\n  # phi is probabilities of the topics for each of the terms\n\tphi <- posterior(lda_tm)$terms %>% as.matrix\n\t\n\t# theta is probabilities of the topics for each the document\n\ttheta <- posterior(lda_tm)$topics %>% as.matrix\n\t#phi2 <- phi[ ,order(as.integer(colnames(phi)))]\n\tvocab <- colnames(phi)\n\n\tdoc_length <- vector()\n\tdoc_topic_df<-as.data.frame(doc_topic)\n\n\t#get document length\n\tfor( i in as.numeric(row.names(doc_topic_df))){\n\t  temp <- corp[[i]]$content\n\t  doc_length <- c(doc_length, nchar(temp[2]))\n\t}\n\n\ttemp_frequency <- as.matrix(new_dtm)\n\n\tfreq_matrix <- data.frame(ST = colnames(temp_frequency),\n\t                          Freq = colSums(temp_frequency))\n\n\tjson_lda <- createNamJson(phi = phi, theta = theta,\n        \t               vocab = vocab,\n                \t       doc.length = doc_length,\n\t                       term.frequency = freq_matrix$Freq,\n        \t               mds.method = canberraPCA)\n\n\t#serVis(json_lda, out.dir = paste(\"/home/ruser/TextPrism/LDAvis_Result/\",name,sparseRe,\"_\",k,sep=\"\"), open.browser = FALSE)\n\t##release to TOMCAT\n\tserVis(json_lda, out.dir = paste(\"/data001/tomcat/webapps/\",name,sparseRe,\"_\",k,sep=\"\"), open.browser = FALSE)\n}\nend.time <- Sys.time()\nsvmtraintime <- end.time - start.time\n\nprint(\"Analysis Time:\")\nprint(svmtraintime)\nprint(\"LDA Complete\")\n\nrm(tm)\nrm(start.time)\nrm(end.time)\n\nsave.image(file=paste(\"/home/ruser/TextPrism/output/\",name,sparseRe,\"_\",k,\"_clustering_LDA_Result.RData\",sep=\"\"))\nprint(paste(\"http://165.243.188.249:8080/\",name,sparseRe,\"_\",k))\n",
    "created" : 1481933967419.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3670454734",
    "id" : "7D59DA29",
    "lastKnownWriteTime" : 1464875378,
    "last_content_update" : 1464875378,
    "path" : "~/GitHub/TextMining/clustering_LDA_v1.62.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}