{
    "collab_server" : "",
    "contents" : "## TM_Keywords\n\nfn_tm_keys <- function(tmResult){\n tm_keys <-\n  tmResult %>% \n  group_by(crawl_data_id) %>%\n  mutate(kw=stri_dup(stri_c(keyword,\"  \"),ranking)) %>%\n  summarise(keywords=stri_c(kw,collapse=\"  \"))\n return(tm_keys)\n}\n\n## TM_Roles\n\nfn_tm_roles <- function(tmResult){\n tm_roles <-\n  tmResult %>%\n  group_by(crawl_data_id, role) %>%\n  summarise(sumCount=sum(ranking))\n \n tm_roles <- cast(tm_roles, crawl_data_id ~ role, sum)\n tm_roles <- subset(tm_roles, select=c(crawl_data_id,A1,A2,AJ,AZ,NB,NN, NZ,url,VB))\n return(tm_roles)\n}\n\n## TM_Doc_Type\nfn_tm_target <- function(tmResult){\n tm_doc_type <- tmResult %>% group_by(crawl_data_id, target) %>% summarise(sumCount=sum(ranking))\n tm_doc_type <- subset(tm_doc_type, select=(-sumCount))\n return(tm_doc_type)\n}\n\n## Make DTM\nfn_makeDTM <- function(tm_keys,sparseTerm){\n corp <- Corpus(DataframeSource(tm_keys))\n dtm <- DocumentTermMatrix(corp, \n                           control=list(removeNumbers=TRUE, \n                                        wordLengths=c(2,Inf)))\n dtm <- removeSparseTerms(dtm, sparseTerm)\n dtmDf <- as.data.frame(as.matrix(dtm))\n return(dtmDf)\n}\n\n## Make TF-IDF DTM\nfn_makeTfIdfDTM <- function(tm_keys,sparseTerm){\n corp <- Corpus(DataframeSource(tm_keys))\n dtm <- DocumentTermMatrix(corp,\n                           control=list(weighting=weightTfIdf,\n                                        removeNumbers=TRUE,\n                                        wordLengths=c(2,Inf)))\n dtm <- removeSparseTerms(dtm, sparseTerm)\n dtmDf <- as.data.frame(as.matrix(dtm))\n return(dtmDf)\n}\n\n## Make TF-IDF DTM2\n##Remove low tf-idf col and row\nfn_makeTfIdfDTM2 <- function(tm_keys, sparseTerm, i){\n  corp <- Corpus(DataframeSource(tm_keys))\n  dtm <- DocumentTermMatrix(corp, \n                            control=list(removeNumbers=TRUE, \n                                         wordLengths=c(2,Inf)))\n  dtm <- removeSparseTerms(dtm, sparseTerm)\n  \n  term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))\n  new_dtm <-dtm[,term_tfidf >= i]\n  new_dtm <-new_dtm[row_sums(new_dtm)>0,]\n  \n  dtmDf <- as.data.frame(as.matrix(new_dtm))\n  return(dtmDf)\n}\n\n\n\n##LDA_Result_change_for Qlikview\nfn_LDA_Result_for_QV <- function(term_topic){\n temp <-NULL\n output<-NULL\n\n for(i in 1:ncol(term_topic)){\n   for(j in 1:nrow(term_topic)){\n    temp$topicNo <- i\n    temp$keyword <- term_topic[j,i]\n    output <-  rbind(output,temp)\n    }\n  }\n \n return(output)\n}\n\n##LDA_Result_change_for Qlikview\nfn_LDA_term_Result_for_QV <- function(phi){\n temp <-NULL\n output<-NULL\n\n for(i in 1:ncol(phi)){\n   for(j in 1:nrow(phi)){\n    temp$topicNo <- j\n    temp$keyword <- noquote(colnames(phi)[i])\n    temp$termProb <- phi[j,i]\n    output <-  rbind(output,temp)\n    }\n  }\n return(output)\n}\n\n\n##Build a Graph\nmakeNetwork <- function(networkMatrix){\n  g <- network(networkMatrix, weighted=T, mode = \"undirected\")\n  g <- simplify(g)\n  V(g)$label <- V(g)$name\n  V(g)$degree <- degree(g)\n  \n  set.seed(3952)\n  layout1 <- layout.kamada.kawai(g)\n  layout2 <- layout.graphopt(g)\n  plot.igraph(g, layout=layout1,edge.width=2, main=\"TopicKeword Network\", vertex.size=10*sqrt(hub.score(g)$vector))\n  \n  write.graph(g,file =\"test.ncol\",format=\"ncol\")\n}\n\n",
    "created" : 1481934347948.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2173636941",
    "id" : "D5F18BBD",
    "lastKnownWriteTime" : 1462507659,
    "last_content_update" : 1462507659,
    "path" : "~/GitHub/TextMining/ML_functions.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}